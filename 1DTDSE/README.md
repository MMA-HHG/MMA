# CTDSE
CTDSE is a 1-dimensional time-dependent Schr√∂dinger equation solver (1D-TDSE) written in C language. The purpose of the code is to evaluate the microscopic response, i.e. the observables, during the propagation of the electric field. The solver is based on the grid method and the propagation scheme employs operator splitting and Crank-Nicolson method. 

The code offers multiple binaries and interactive interfaces for various tasks:
* computation of microscopic responses in the whole medium for cylindrically symmetrical macroscopic fields from CUPRAD,
* evaluation of a single microscopic response for only one field from from the macroscopic CUPRAD field
* interactive Python interface wrapping C routines for the propagation enabling advanced analysis of the microscopic response as well as the ability to impose arbitrary electric field. 

# Table of contents
1. [Install](#install)
   1. [Dependencies](#dependencies)
   2. [Setup environment variables](#setup-environment-variables)
   3. [CMake installation](#cmake-installation)
   4. [Advanced compilation options](#advanced-compilation-options)
   5. [Python-TDSE library compilation](#python-tdse-library-compilation)
2. [User Guide](#user-guide)
   1. [MPI scheduler for the CUPRAD output](#mpi-scheduler-for-the-cuprad-output)
   2. 

# Install

## Dependencies
Before installing CTDSE, a few dependencies are required:
* a **C compiler** (GNU/Intel),
* **CMake** utility,
* an **MPI** library,
* an **HDF5** library,
* an **FFTW3** library (for GNU) OR corresponding **MKL** library containing FFTW3 libraries compatible with Intel C compiler,
* **Python** (3.+) with *h5py, ctypes, numpy, matplotlib* modules.
* **Universal inputs** Python module (see README in the home directory of CUPRAD_TDSE_Hankel)

## Setup environment variables

On HPC the variables are set by loading the corresponding modules, e.g.
```shell
module load cmake gcc openmpi fftw3 hdf5
```
If the modules are selected and installed properly (compatible modules), then the variables ```LD_LIBRARY_PATH``` and ```CPATH``` should include the paths to the dependency libraries and header files.

### Known issues with environment variables
For **Intel compiler** and **MKL** binding, the ```CPATH``` variable may be set incorrectly and does not include the correct path to FFTW3 headers. The error will show up during the compilation phase, stating that the header ```<fftw3.h>``` is missing. 

If the problem occurs, the workaround is to set the correct include path for the MKL header either by editing the ```.bashsrc``` file or modifying the ```CPATH``` variable in the current session as follows:
```shell
export CPATH=${CPATH}:${MKLROOT}/include/fftw
```
where ```MKLROOT``` is a variable pointing towards the MKL installation and is set on HPC by loading the module MKL.

## CMake installation
CMake utility is used to build a Makefile that eventually builds the CDTSE code. 

1. First move into the CTDSE root directory and create a new directory ```build``` and move into this directory as follows
```shell
# PWD = ../CUPRAD_TDSE_Hankel/1DTDSE
mkdir build
cd build
```
2. The next step involves invoking ```cmake``` command and specification of the compiler. For GNU compiler do the following:
```shell
# GNU Compiler = mpicc
cmake -D CMAKE_C_COMPILER=mpicc ..
```
For Intel compiler do:
```shell
# Intel Compiler = mpiicc
cmake -D CMAKE_C_COMPILER=mpiicc ..
```

If all the environment variables were set properly (see section [Setup environment variables](#setup-environment-variables)) or modules loaded correctly, CMake should generate a new ```Makefile``` file in the ```build``` directory. 

3. The code is then compiled with the ```make``` command:
```shell
# PWD = ../CUPRAD_TDSE_Hankel/1DTDSE/build
make
```
4. If no error was displayed, the following files were generated by the makefile: ```TDSE.e```, ```TDSE_stride.e```, ```libsingleTDSE.so``` (or ```libsingleTDSE.dylib``` dependending on the system)

### Built targets
* ```TDSE.e```: MPI scheduler for execution of the parallel computation of the microscopic response for the field from CUPRAD,
* ```TDSE_stride.e```: serial code for running 1D-TDSE on a single output from the CUPRAD field. 
* ```libsingleTDSE.so```: core C dynamic library for running the 1DTDSE from the interactive Python environment. 
<!--
## Advanced compilation options

## Python-TDSE library compilation
-->

# User guide

## MPI scheduler for the CUPRAD output
MPI scheduler takes the output field obtained from the previous computation of the CUPRAD code. The scheduler reads fields from HDF5 archive and for each field in $(r, z)$ coordinate executes a single 1DTDSE propagation routine. Each process stores the result data in its dedicated temporary HDF5 archive, e.g. for process 1 it will be ```hdf5_temp_0000001.h5```. 

### Preprocessing of the HDF5 input

### Code execution

### Simple Slurm script example

## Single field caller

## Python-TDSE wrapper



# Development

## Goals
TDSE is used as a part of the multi-scale model. It is also usefull itself for various solely microscopic studies. Furthemore, it might have more usages for the macroscopic studies: 1) to process numerical fields from CUPRADS, 2) to create a list of microscopic currents that are used to intepolate the HHG sources in a macroscopic medium.

The code shall be well-organised for all these tasks: core-rutines are written only once and are called from different programs. The architecture is the following: the solver itselfs works as a function `inputs` $\to$ `outputs`. This is called from the driving programs for each task.

We provide two opssible operations in the publication:
* MPI processing of the numerical field
* A single caller to treat microscopic problems

### Single TDSE for versatile applications
* Python wrapper to run the code easily from jupyter notebooks etc.
* Reintroduce storing the wavefunction as an output of TDSE for detailed analysis
* (?) Use Gabor transform from c, ...

## Improvements
* The single caller is separated in `single_caller.c` and `prop.c`. The former just unpacks and packs I/O structures to feed the caller. I think this is a bit clumsy and might be improved.
* The main part of the multi-scale model is the MPI scheduler. We need to comment and polish it.
	* The scheduler now works in a strided regime: zeroth processors gets tasks $0,N_{\text{procs}},2N_{\text{procs}},\dots$, first processor $1,N_{\text{procs}}+1,2N_{\text{procs}}+1,\dots$, ... (See `strided_scheduler*.c`.)
	* Current implementation uses a hdf5-output for each processor. Results are merged by Python code `post_processing/merge*.py`. We might use a collective access similarly to CUPRAD to store outputs directly.
	* The opearation still doesn't seem optimal. Some processors end their work much later than the others (the tasks are similar & by repated runs for the same configurations, it were different processors). It seem some subtasks are locked for some time (but not deadlocked). This was difficult to trace, the scale of the task was also a possible issue.
	* It seem to me that an elegant design would be to use a shared counter to assign tasks to idle processors. This functionality was introduced in the MPI3 standard as is provided as an example `NXTVAL` in the reference book[^1]. I was trying to implement it (see `hdf5+mpi_tuto/*`, `counting.c`, `counting2.c`), but it was unstable and problematic. We try to find someone experienced in MPI3 and discuss it.


[^1]: Gropp, Hoefler, THakur, Lusk *Using Advanced MPI*


# Minimalistic user-guide
This folder cointais a collection of *.c codes and a Python script used for

- MPI-scheduler that computes 1DTDSEs for all electric fields in an (r,z)-grid.
- Single 1DTDSE computed for one input.
- Python script collects all results from temporary files generated by the scheduler.

! It is still in experimental version. Mainly, the input units are not resolved. Next, it is hard-coded that code prints all available outputs, but the method for selecting some of them is already presented.

## MPI-scheduler
An example of a compilation and submission scripts:
<pre>
#!/bin/bash

#module purge
#module load intel intelmpi fftw3 hdf5

# ECLIPSE-ELI
#module purge
#module load GCC/6.1.0 OpenMPI/1.10.2-GCC-6.1.0-psm-dbg HDF5/1.8.13-OpenMPI-1.10.2-GCC-6.1.0-psm FFTW/3.3.5-OpenMPI-1.10.2-GCC-6.1.0-psm-dbg

# OCCIGEN
# module purge
module purge
module load intel/18.1 intelmpi/2018.1.163 hdf5/1.10.4
module load fftw/3.3.8
# module load intel/18.1 intelmpi/2018.1.163
# module load intel/19.4 intelmpi/2019.4.243
# module load intel/19.4 openmpi/intel/2.0.4
# module load gcc/8.3.0 openmpi/gnu/2.0.4

module list

rm *.h5 *.e sendjob *.output *.log

cp ../tx/results.h5 .
cp ../tx/results.h5 results2.h5

mv TDSEScheduler-*out oldlogs/
mv logfile.log-* oldlogs/

which h5pcc 
export HDF5_USE_SHLIB=yes # default static lib fore h5pcc
h5pcc -c $HOME/git/1DTDSE/constants.c $HOME/git/1DTDSE/tools_algorithmic.c $HOME/git/1DTDSE/tools_MPI-RMA.c $HOME/git/1DTDSE/tools_hdf5.c $HOME/git/1DTDSE/tridiag.c $HOME/git/1DTDSE/tools_fftw3.c $HOME/git/1DTDSE/tools.c $HOME/git/1DTDSE/prop.c $HOME/git/1DTDSE/singleTDSE.c $HOME/git/1DTDSE/strided_scheduler_separated_outs.c -L$HDF5_LIBDIR -lhdf5 -lhdf5_hl -lfftw3
h5pcc *.o -L$HDF5_LIBDIR -lhdf5 -lhdf5_hl -lm -lfftw3 -o test_strided2.e
rm *.o


echo 'compilation finished'
</pre>

<pre>
#!/bin/bash
#SBATCH -J 1DTDSE_MPI
#SBATCH  --constraint=HSW24  
#SBATCH  --exclusive
#SBATCH  --nodes=1
#SBATCH  --ntasks=2
#SBATCH  --ntasks-per-node=24 
#SBATCH  --threads-per-core=1
#SBATCH  --time=00:30:00 
#SBATCH  --output=TDSEScheduler-%J.out
module purge
module load intel/18.1 intelmpi/2018.1.163 hdf5/1.10.4 
module load fftw/3.3.8
# module load intel/19.4 intelmpi/2019.4.243
# module load intel/19.4 openmpi/intel/2.0.4
# module load gcc/8.3.0 openmpi/gnu/2.0.4

export I_MPI_PIN_PROCESSOR_LIST=allcores:map=spread
export SLURM_CPU_BIND=none
#
export I_MPI_DAPL_PROVIDER=ofa-v2-mlx5_0-1s
export I_MPI_DAPL_DIRECT_COPY_THRESHOLD=655360
# 
export I_MPI_PIN=1
export I_MPI_DEBUG=6

ldd ./test_strided2.e

time  mpirun -n $SLURM_NTASKS ./test_strided2.e > logfile.log-$SLURM_JOBID
</pre>

## Single 1DTDSE
The main feature is that this code shares the source with MPI. It can thus be used for development tuning that is then directly available in MPI, e.g. I used valgrind for this code to check that there are no internal leaks within the procedure (all possible problems are then intrinsic to the MPI-Scheduler).
<pre>
#!/bin/bash

#module purge
#module load intel intelmpi fftw3 hdf5

# ECLIPSE-ELI
#module purge
#module load GCC/6.1.0 OpenMPI/1.10.2-GCC-6.1.0-psm-dbg HDF5/1.8.13-OpenMPI-1.10.2-GCC-6.1.0-psm FFTW/3.3.5-OpenMPI-1.10.2-GCC-6.1.0-psm-dbg

# OCCIGEN
module purge
module load intel/18.1 intelmpi/2018.1.163 hdf5/1.10.4
module load fftw/3.3.8

rm *.h5 *.e sendjob *.output *.log results/*

cp ../tx/results_single_short.h5 results.h5
cp ../tx/results_single_short.h5 results2.h5
which h5pcc 
export HDF5_USE_SHLIB=yes # default static lib fore h5pcc
h5pcc  -c $HOME/git/1DTDSE/constants.c $HOME/git/1DTDSE/tools_hdf5.c $HOME/git/1DTDSE/tridiag.c $HOME/git/1DTDSE/tools.c $HOME/git/1DTDSE/tools_fftw3.c $HOME/git/1DTDSE/tools_algorithmic.c $HOME/git/1DTDSE/prop.c $HOME/git/1DTDSE/singleTDSE.c $HOME/git/1DTDSE/single_caller.c -L$HDF5_LIBDIR -lhdf5 -lhdf5_hl -lfftw3 -g
h5pcc --echo  *.o -L$HDF5_LIBDIR -lhdf5 -lhdf5_hl -lm -lfftw3 -g -o test_s1.e
rm *.o


echo 'compilation finished'
</pre>

It can be run in an interactive mode using
<pre>
module load intel/18.1 intelmpi/2018.1.163 hdf5/1.10.4; module load fftw/3.3.8

srun test_s1.e
</pre>


## Python post-processor
Because there is no stable direct MPI-Scheduler, there is a post-processor connectiong all files, it is in the folder **post_processing**. It is general and it can be easily modiable for other purposes.

At the instant, it uses hdf5-oprations that are sourced from *mynumerics.py* from the Hankel-transform repo.



# Development notes
## The plot of the code:

1. See the comment at the end of the file for deatils about the MPI-scheduler.

2. The main program (MP) reads all the parameters from an input HDF5-archive (each process independently, read-only).

3. MP decides based on parameters the type of input field. Fist implementation: stick to numerical fields from CUPRAD stored in the archive.

4. MPI-scheduler executes a simulation in every point. THe data are directly written into the output using the mutex.

5. The code finishes.

- DEVELOPMENT: wrap the call of singleTDSE, where propagation is called to test, erase this extra step after

!!! There is memory allocated in every TDSE. Check there are no memory leaks, it matters now.

## Local problems:
single_caller.c is a program to call a single TDSE simulation. The input is one numerical field at the instant.


## Extensions/features already presented in 1DTDSE
THere is a list of features we added in the code throughout the time, we don't have them in the sheduler. But it would be worthy to re-introduce them simply by modifying the output structure.

- The features already presented in TDSE:
	1. Print wavefunction (WFT)
	2. Print Gabor transformation
	3. Photoelectron spectrum (PeS, Fabrice)
  4. Various outputs modifications (e.g. ionisation filtering done for Sophie)

Gabor is computationally demanding; WF and PeS are both data-storage demandig. It should be used then only in few prescribed points.


--There is possibility of various inputs:
	1. Numerical/analytic field
	2. Computation on velocity/length gauge

We have already an analytic model of a beam (Python/MATLAB), we will then rewrite it and construct the parameters on-the-fly.
The versatility in numeric-or-analytic field length-or-velocity gauge is ideal for testing of numerical vector potential that we can use after in SFA.


## Development notes:
<pre>
0) Use void*-types for inputs and link it with the required ouput precision.

1) We can use checks whether parameters exist in the input HDF5-archive. If not, we can create them with default values.
Implementation: since this is I/O operation with one file, we need r/w. Maybe read paramc only by proc 0 and the broadcast structure (see the MPI book for transfering structs).

For reading, it should be easy. ** R/W may occur simultaneously in in the MPI loop. Separate I/O at the instant or ensure it will work (R/W from independent datasets may be fine???).
https://support.hdfgroup.org/HDF5/Tutor/selectsimple.html

2) Actual construction does some preparations within 1 TDSE simulation, it is desirable to separate preparations and core propagation, it's then more versatile for the MPI-calling.
2.solution) separate all preparatory work from singleTDSE into one procedure, called pecedently the calculations, the "pure" propagation then wites into its template.
2.develop) keep in mind that this approach may not be possible every time and one run has to be done before alligning it... Try to code it clever way.

3) The code is inconsistent. SOme outputs from the first version are listed as independent variables and not encapsulated in structures. Fix it.

4) There is a "better" mutex proposed in the MPI3-book, ref. [44] therein. Try to implement it.
4.add) The shared mutex seems to be wrong by some reason.

5) we get rid of mutexes and use rather parallel acces to files all the time.
5.develop) it seems that many-readers many-writers would be possible by HDF5 parallel since we will not modify the file much. However, we may also try stick with independent files and eventually 
https://stackoverflow.com/questions/49851046/merge-all-h5-files-using-h5py
https://portal.hdfgroup.org/display/HDF5/Collective+Calling+Requirements+in+Parallel+HDF5+Applications
</pre>

## Valgrind
<pre>
	  for(i=0;i<=num_r;i++)
	  {
		  dinfnew[2*i] = dinf[2*i] - Eguess/12. + potential(x[i],trg)/12.; dinfnew[2*i+1] = dinf[2*i+1];
		  dnew[2*i] = 10*potential(x[i],trg)/12.+ d[2*i] - 10*Eguess/12.; dnew[2*i+1] = d[2*i+1];
		  dsupnew[2*i] = dsup[2*i] - Eguess/12. + potential(x[i+1],trg)/12.; dsupnew[2*i+1] = dsup[2*i+1];
		  diag[2*i] = potential(x[i],trg)+ d[2*i] ; diag[2*i+1] = d[2*i+1];
	  }
</pre>
wrongly specified x(n+1) element. Hotfixed.

<pre>
		for(j = 0 ; j<= num_r ; j++) 
		{	
			dinfnew1[2*j] = 1/12.; dinfnew1[2*j+1] = 0.5*dt*( -0.5/(dx*dx) )+0.5*dt*1/12.*(cpot*potential(x[j],trg));
			dnew1[2*j] = 10/12.; dnew1[2*j+1] = 0.5*dt*( 1./(dx*dx) )+0.5*dt*10/12.*(cpot*potential(x[j],trg));
			//dsupnew1[2*j] = 1/12.; dsupnew1[2*j+1] = 0.5*dt*( -0.5/(dx*dx) )+0.5*dt*1/12.*(cpot*potential(x[j+1],trg));			
		}
</pre>
dtto


<pre>
Efield.Field = FourInterp(k1, Efield.Field, Efield.Nt); // make the interpolation !!!!!! tgrid does not correspond any more
</pre>
tricky since a pointer may be lost

