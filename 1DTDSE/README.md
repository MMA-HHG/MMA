<!-- omit in toc -->
# CTDSE
CTDSE is a 1-dimensional time-dependent Schrödinger equation solver (1D-TDSE) written in C language. The purpose of the code is to evaluate the microscopic response, i.e. the observables, during the propagation of the electric field. The solver is based on the grid method and the propagation scheme employs operator splitting and the Crank-Nicolson method. 

The code offers multiple binaries and interactive interfaces for various tasks:
* **MPI-CTDSE**: computation of microscopic responses in the whole medium for cylindrically symmetrical macroscopic fields from CUPRAD,
* **single-CTDSE**: evaluation of a single microscopic response for only one field from the macroscopic CUPRAD field
* **CTDSE as a dynamic library**: interactive Python interface wrapping C routines for the propagation enabling advanced analysis of the microscopic response as well as the ability to impose arbitrary electric field. 

The default recipe allows the installation of all the binaries. We provide also a simpler installation for the *CTDSE as a dynamic library*.

<!-- omit in toc -->
# Table of contents
- [Install CTDSE as a part of the multiscale model](#install-ctdse-as-a-part-of-the-multiscale-model)
	- [Dependencies](#dependencies)
	- [Setup environment variables](#setup-environment-variables)
		- [Known issues with environment variables](#known-issues-with-environment-variables)
	- [CMake installation](#cmake-installation)
		- [Built targets](#built-targets)
- [Install only the dynamic CTDSE library](#install-only-the-dynamic-ctdse-library)
	- [Dependencies](#dependencies-1)
	- [CMake installation](#cmake-installation-1)
- [Test installation](#test-installation)
- [User guide](#user-guide)
	- [MPI-TDSE for the CUPRAD output](#mpi-tdse-for-the-cuprad-output)
		- [Preprocessing the HDF5 input](#preprocessing-the-hdf5-input)
		- [MPI-TDSE code execution](#mpi-tdse-code-execution)
		- [HDF5 temporary files merge](#hdf5-temporary-files-merge)
		- [Batch job pipeline for MPI-TDSE](#batch-job-pipeline-for-mpi-tdse)
	- [Single field caller](#single-field-caller)
	- [Python-TDSE wrapper](#python-tdse-wrapper)

# Install CTDSE as a part of the multiscale model
## Dependencies
Before installing CTDSE, a few dependencies are required:
* a **C compiler** (GNU/Intel),
* **CMake** utility,
* an **MPI** library,
* an **HDF5** library,
* an **FFTW3** library (for GNU) OR corresponding **MKL** library containing FFTW3 libraries compatible with Intel C compiler,
* **Python** (3.+) with *h5py, ctypes, numpy, matplotlib* modules.
* **Universal inputs** Python module (see README in the home directory of CUPRAD_TDSE_Hankel)

**NOTE**: MPI library is not required for **single-CTDSE** code. *If MPI is not found on the system*, the following steps apply, however, **MPI-CTDSE** *will not be built!*

## Setup environment variables

On HPC the variables are set by loading the corresponding modules, e.g.
```shell
module load cmake gcc openmpi fftw3 hdf5
```
If the modules are selected and installed properly (compatible modules), then the variables ```LD_LIBRARY_PATH``` and ```CPATH``` should include the paths to the dependency libraries and header files.

### Known issues with environment variables
For the **Intel compiler** and **MKL** binding, the ```CPATH``` variable may be set incorrectly and may not include the correct path to the FFTW3 headers. The error will show up during the compilation phase, stating that the header ```<fftw3.h>``` is missing. 

If the problem occurs, the workaround is to set the correct include path for the MKL header either by editing the ```.bashsrc``` file or modifying the ```CPATH``` variable in the current session as follows:
```shell
export CPATH=${CPATH}:${MKLROOT}/include/fftw
```
where ```MKLROOT``` is a variable pointing towards the MKL installation and is set on HPC by loading the module MKL.

## CMake installation
The CMake utility is used to build a Makefile that eventually builds the CDTSE code. 

1. First move into the CTDSE root directory, create a new directory ```build```, and move into this directory as follows
   ```shell
   # PWD = ../CUPRAD_TDSE_Hankel/1DTDSE
   mkdir build
   cd build
   ```
2. The next step involves invoking the ```cmake``` command and specification of the compiler. For the GNU compiler do the following:
   ```shell
   # GNU Compiler = mpicc
   cmake -D CMAKE_C_COMPILER=mpicc ..
   ```

   For the Intel compiler do:
   ```shell
   # Intel Compiler = mpiicc
   cmake -D CMAKE_C_COMPILER=mpiicc ..
   ```

   If all the environment variables were set properly (see section [Setup environment variables](#setup-environment-variables)) or modules loaded correctly, CMake should generate a new ```Makefile``` file in the ```build``` directory. 

3. The code is then compiled with the ```make``` command:
	```shell
	# PWD = ../CUPRAD_TDSE_Hankel/1DTDSE/build
	make
	```
4. If no error was displayed, the following files were generated by the makefile: ```TDSE.e```, ```TDSE_stride.e```, ```libsingleTDSE.so``` (or ```libsingleTDSE.dylib``` depending on the system)

**NOTE**: ```CMAKE_C_COMPILER``` variable can also be set to ```gcc``` (for GNU compiler) or ```icc``` (for Intel compiler). This is necessary for building only the **single-CTDSE** and the **CTDSE as a dynamic library** without an MPI library (necessary for **MPI-CTDSE**).

### Built targets
* ```TDSE.e``` (**MPI-CTDSE**): MPI scheduler for the execution of the parallel computation of the microscopic response for the field from CUPRAD,
* ```TDSE_stride.e``` (**Single-CTDSE**): serial code for running 1D-TDSE on a single output from the CUPRAD field. 
* ```libsingleTDSE.so``` (**CTDSE as a dynamic library**): core C dynamic library for running the 1DTDSE from the interactive Python environment. 
<!--
## Advanced compilation options

## Python-TDSE library compilation
-->


# Install only the dynamic CTDSE library

For some use cases, it might be sufficient to only compile the CTDSE library for running the code on a local machine without the necessity of having MPI or HDF5 installed.

## Dependencies
* a **C compiler** (GNU/Intel),
* **CMake** utility,
* an **FFTW3** library (for GNU) OR corresponding **MKL** library containing FFTW3 libraries compatible with Intel C compiler,
* **Python** (3.+) with *h5py, ctypes, numpy, matplotlib* modules.

## CMake installation
1. Replace ```CMakeLists.txt``` with ```CMakeLists_dll_only.txt``` which contains the following:
	```cmake
	### CMake file for The dynamic 1D-TDSE library
	cmake_minimum_required(VERSION 3.13)

	project(TDSE)
	enable_language(C)      

	message("Compiler: ${CMAKE_C_COMPILER}")
	message("Compiler ID: ${CMAKE_C_COMPILER_ID}")

	if(${CMAKE_C_COMPILER_ID} MATCHES Intel)
		set(MKL_INCLUDE_DIRS ${MKLROOT}/include/fftw)
		set(MKL_LIB_DIRS ${MKLROOT}/lib)
		set(FFTW_LIBS mkl_gf_lp64 mkl_sequential mkl_core)
	elseif(${CMAKE_C_COMPILER_ID} MATCHES GNU)
		find_library(fftw3 fftw3)
		set(FFTW_LIBS fftw3) 
	elseif(${CMAKE_C_COMPILER_ID} MATCHES AppleClang)
		find_library(fftw3 fftw3)
		set(FFTW_LIBS fftw3 fftw3_mpi)
	else()
		message(FATAL_ERROR "Unsupported C compiler: ${CMAKE_C_COMPILER}")
	endif()

	set(SOURCE_DLL sources/constants.c sources/tools_algorithmic.c 
		sources/tridiag.c sources/tools_fftw3.c sources/structures.c 
		sources/tools.c sources/prop.c sources/singleTDSE.c)

	set(TDSE_LIB singleTDSE)

	add_library(${TDSE_LIB} SHARED ${SOURCE_DLL})
	target_link_libraries(${TDSE_LIB} PRIVATE ${FFTW_LIBS} ${HDF5_LIBRARIES})
	target_include_directories(${TDSE_LIB} PRIVATE ${HDF5_INCLUDE_DIRS})
	target_link_options(${TDSE_LIB} PRIVATE -fPIC)
	```
2. Move into the CTDSE root directory to create a new directory ```build``` and move into this directory:
   ```shell
   # PWD = ../CUPRAD_TDSE_Hankel/1DTDSE
   mkdir build
   cd build
   ```
2. The next step involves invoking the ```cmake``` command and specification of the compiler. For the GNU compiler do the following:
   ```shell
   # GNU Compiler = gcc
   cmake -D CMAKE_C_COMPILER=gcc ..
   ```

   For the Intel compiler do:
   ```shell
   # Intel Compiler = icc
   cmake -D CMAKE_C_COMPILER=icc ..
   ```

   If all the environment variables were set properly (see section [Setup environment variables](#setup-environment-variables)) CMake should generate a new ```Makefile``` file in the ```build``` directory. 

3. The code is then compiled using the ```make``` command:
	```shell
	# PWD = ../CUPRAD_TDSE_Hankel/1DTDSE/build
	make
	```
4. If no error was displayed, the dynamic library (DLL) ```libsingleTDSE.so``` (or ```libsingleTDSE.dylib``` depending on the system) has been created.


<!-- omit in toc -->
### Local installation (Ubuntu 18.04 subsystem) 
``apt-get install build-essential``
``apt-get install libhdf5-dev``
`` sudo apt-get install hdf5-helpers ``
cmake may have problems with h5cc (worked with v3.15.7)
check the fftw3 installation (`-fPIC`)

# Test installation
The validity of the installation can be checked using the Python `unittest` module. The tests are implemented within the `tests.py` file. After successful compilation of the C-TDSE dynamic library `libsingleTDSE.dll` located in `build/`, run the script as follows with command line argument `-d` or `--dll`, which specifies the path to the DLL, to check the results:
```bash
python tests.py -d build/libsingleTDSE.dll
```
The test executes a single 1D-TDSE computation and compares the results with precomputed dataset `ionization.h5`.

# User guide

## MPI-TDSE for the CUPRAD output
MPI-TDSE scheduler takes the output field obtained from the previous computation of the CUPRAD code. The scheduler reads the fields from an HDF5 archive and for each field in $(r, z)$ coordinate executes a single 1DTDSE propagation routine. Each process stores the result data in its dedicated temporary HDF5 archive, e.g. for process 1 it will be ```hdf5_temp_0000001.h5```. 

Before the execution of the MPI-TDSE, we must preprocess the output ```results.h5``` file from CUPRAD.

### Preprocessing the HDF5 input
The file ```results.h5``` is, by default, stored within the ```CUPRAD/build/``` directory after the execution of CUPRAD. It will be copied and preprocessed from this directory by default (can be overridden). 

<!-- omit in toc -->
#### Parameter file
The input file `TDSE_input_params.inp` is an example input file containing numerical parameters for the execution of the MPI-TDSE code. **Be advised to keep a backup of this file before overwriting it with custom parameters**. 
The file has a predefined structure along with explanatory comments for each parameter and looks as follows:
```
## Example TDSE input file used for preprocessing of HDF5 file
## ------------------
## This file is read by the python preprocessor ```post_processing/prepare_TDSE.py```
## and saves the values stored here into the HDF5 archive into the group 
## "TDSE_inputs/". 
##
## Brief explanation:
## HDF5_variable_name  |  <value>  |  <type: real (R), integer (I)>  |  <units>  |  # comment

## Coarser grid parameters
kz_step 1  I   -
# Nz_max  1250 I   -
kr_step 4  I   -
Nr_max  400 I   -

## Control outputs: here we set the outputs we would like to save into 
## the temporary HDF5 file (1 == keep, 0 == not keep)
print_GS_population             1   I   -   
print_integrated_population     1   I   -
print_Efield                    1   I   -
print_F_Efield                  0   I   -
print_Source_Term               0   I   -
print_F_Source_Term             1   I   -
print_x_expectation_value       1   I   -
print_GS                        1   I   -
print_F_Efield_M2               0   I   -
print_F_Source_Term_M2          0   I   -

## Numerical and starting parameters
Eguess	-1	R	a.u.	# Energy of the initial state (guess)
N_r_grid	16000	I	-	# Number of points of the initial TDSE spatial grid 
dx	0.4	    R	a.u.	# Resolution for the grid
x_int	2.0	R	a.u.    # Probability for finding electron in range (x-x_int, x + x_int)
CV_criterion_of_GS  1e-25   R   -   # Convergence parameter for ground state search
gauge_type  0   I   -   # Choice of gauge (0 == length) <-- other gauges NOT IMPLEMENTED
InterpByDTorNT	0	I	-	# Refine resolution only for numerical fields (0 - by dt, 1 - by number of points)
dt	0.25	R	a.u.	# Resolution in time
Ntinterp	1	I	-	# Number of intermediate points for the interpolation

## Target definition
trg_a	1.3677	R	a.u. # |Krypton Ip = 0.5145 a.u. a = 1.3677, Argon Ip = 0.5792 a = 1.1893
```

<!-- omit in toc -->
##### Coarser grid parameters
The parameters `Nz_max` and `Nr_max` for grid coarsening are optional. Enables to coarsen the CUPRAD electric grid for the computation, decreasing the resolution for the MPI-TDSE.

<!-- omit in toc -->
##### Control outputs
Enables the selection of which parameters should be stored in the temporary HDF5 files generated per process containing the results of MPI-TDSE. 

<!-- omit in toc -->
##### Numerical and starting parameters
The notes are self-explanatory. **Be advised to always check the TDSE parameters such as `dx`, `dt`, and `N_r_grid` for a proper wavefunction convergence, the code does not check if the resolution is sufficient!** Note that the `gauge` switch has not been implemented in the current version of the code. 


<!-- omit in toc -->
#### Saving the parameters into an HDF5 file
After choosing the desired parameters in the input file, simply run the Python script ```prepare_TDSE.py``` in the ```post_processing``` directory from the root TDSE directory using command line keywords `-i (--paramfile)` (.inp parameter file)  and `-o (--outhdf5)` (.h5 output file) as follows:
```bash
# PWD = ../CUPRAD_TDSE_Hankel/1DTDSE
python3 post_processing/prepare_TDSE.py -i input_parameters.inp -o output_archive.h5
```

For help with the script execution, the user can receive a help message simply by
```bash
# PWD = ../CUPRAD_TDSE_Hankel/1DTDSE
python3 post_processing/prepare_TDSE.py -h
```

The script will create a copy of the HDF5 file in the directory from which the script was called. If the output hdf5 file is in the same directory as the .h5 results file from CUPRAD, the TDSE inputs are appended to the archive.

We can also preprocess the HDF5 file using a batch job Slurm script as follows:
```bash
# PWD = ../CUPRAD_TDSE_Hankel/1DTDSE
sbatch --export=ALL scripts/prepare_TDSE.sh -i input_parameters.inp -o output_archive.h5 -s
```
The option ```--export=ALL``` is necessary for passing the exported variables from the current terminal instance to the batch job. The `-s (--slurm)` option tells the script it is running within the Slurm environment and needs to load the corresponding Python modules. You can again invoke `-h` command for help.

<!-- omit in toc -->
#### Check the HDF5 file before the execution of the MPI-TDSE – non-obligatory
To check if all the important parameters have been printed successfully by the Python preprocessor, the script ```hdf5_check.py``` skims through the HDF5 file to find the corresponding parameters necessary for running the MPI-TDSE. It tells the user which parameters are missing. The script can be executed in two ways. Either through command line arguments:
```bash
# PWD = ../CUPRAD_TDSE_Hankel/1DTDSE
python3 hdf5_check.py -i result_file_to_check.h5
```
or by simply running the script without the CL argument as
```bash
# PWD = ../CUPRAD_TDSE_Hankel/1DTDSE
python3 hdf5_check.py
```
where the script will interactively ask explicitly for the file to be checked:
```bash
Type the HDF5 file to check with relative path: 
result_file_to_check.h5
*******************************

All OK.
Check finished.
```

### MPI-TDSE code execution
Now with the HDF5 file already preprocessed, we can execute the MPI-TDSE code as
```bash
# PWD = ../CUPRAD_TDSE_Hankel/1DTDSE
mpirun -np N_proc ./build/TDSE.e
```
or using a batch job Slurm script ```MPI_TDSE.sh``` as follows:
```bash
# PWD = ../CUPRAD_TDSE_Hankel/1DTDSE
sbatch --ntasks=N_proc --export=ALL scripts/MPI_TDSE.sh
```
The option ```--export=ALL``` is necessary for passing the exported variables from the current terminal instance to the batch job. The number of tasks ```--ntasks=N_proc``` must be specified.

**Note: the file `msg.tmp` containing the name of the preprocessed CUPRAD file with the TDSE input parameters must be located in the code execution directory!** If no `msg.tmp` file is available, the code exits with an error! The "msg.tmp" contains the HDF5 archive name on the first line:
```
CUPRAD_results_file.h5


```


The MPI-TDSE schedules each process with a section of data from the output electric field computed from CUPRAD and executes independently a 1D-TDSE per field. It is an *embarrassingly parallel* algorithm in principle. 

In the beginning, each process allocates its own temporary output HDF5 file in the format ```hdf5_temp_*.h5``` ending with an ID of the corresponding process. Hence for ```N_proc``` processes, we get the ```N_proc``` number of temporary HDF5 files. The output arrays are preallocated in advance within the MPI-TDSE code and then the code prints the preselected values, see section [Parameter file](#parameter-file), into these arrays. 

### HDF5 temporary files merge
We need to merge the temporary ```N_proc``` results ```hdf5_temp_*.h5``` into a single HDF5 file for further analysis. This is done using the last script ```merge.py``` within the ```post_processing``` folder. We invoke the script from the directory with the temporary HDF5 files as follows:
```bash
# PWD = ../CUPRAD_TDSE_Hankel/1DTDSE
python3 post_processing/merge.py
```
This will automatically align all the output fields into a single large array of a new HDF5 file ```results_merged.h5```.

If we want to save some space, we can extract only selected arrays of data. The merging script accepts specific CL arguments that enable the choice of relevant arrays. We can see help information (using CL argument ```-h```) from the script:
```bash
# PWD = ../CUPRAD_TDSE_Hankel/1DTDSE
python3 post_processing/merge.py -h
usage: merge.py [-h] [-p PRINTDATA [PRINTDATA ...]]

optional arguments:
  -h, --help            show this help message and exit
  -p PRINTDATA [PRINTDATA ...], --printdata PRINTDATA [PRINTDATA ...]
                        Select data to print. Available options are 'Efield',
                        'FEfield', 'SourceTerm', 'FSourceTerm', 'FEfieldM2',
                        'FSourceTermM2', 'PopTot', 'PopInt', 'expval_x'
```
See we can select only a few specific arrays to store: 'Efield', 'FEfield', 'SourceTerm', 'FSourceTerm', 'FEfieldM2', 'FSourceTermM2', 'PopTot', 'PopInt', 'expval_x'. 

To select only 'Efield' and 'SourceTerm', we can type:
```bash
python3 post_processing/merge.py -p Efield SourceTerm
```
which will merge the corresponding outputs into the merged results HDF5 file. By default, it merges all the available outputs.

We can also merge the data using a batch job Slurm script ```merge_hdf5.sh``` as follows
```bash
# PWD = ../CUPRAD_TDSE_Hankel/1DTDSE
sbatch --export=ALL scripts/merge_hdf5.sh -p "Efield SourceTerm" -s
```
The user can specify which variables are stored with the merge operation by the ```-p (--printdata)``` option, the available options as mentioned above. The `-s (--slurm)` option tells the script it is running within the Slurm environment and needs to load the corresponding Python modules. You can again invoke the `-h` command for help.

### Batch job pipeline for MPI-TDSE
To summarize, we provide the following Slurm batch job pipeline for the MPI-TDSE. For start, we assume having the data from CUPRAD saved as `results.h5`, without the TDSE inputs group embedded in. 

1. We create a copy of the parameter file `TDSE_input_params.inp` with the desired input parameters for the MPI-TDSE as `inp_file.inp`.
2. The following bash commands are executed:
	```bash
	### Input file
	inp_tdse_filename=/path/to/inp_file/inp_file.inp 
	### HDF5 archive
	h5_filename=/path/to/res_file/results.h5
	### Number of tasks for the MPI-TDSE, say 32
	ntasks_tdse=32
	### Print data
	printdata="Efield SourceTerm"

	### Preprocess input for MPI-TDSE
	JOB1=$(sbatch --parsable --export=ALL \
        $TDSE_1D_SCRIPTS/prepare_TDSE.sh --inp $inp_tdse_filename --ohdf5 $h5_filename -s)

	### Submit MPI-TDSE
	JOB2=$(sbatch --ntasks=$ntasks_tdse --parsable --export=ALL --dependency=afterok:$JOB1 \
        $TDSE_1D_SCRIPTS/MPI_TDSE.sh)

	### Merge TDSEs
	JOB3=$(sbatch --parsable --export=ALL --dependency=afterok:$JOB2 \
        $TDSE_1D_SCRIPTS/merge_hdf5.sh -p $printdata -s)
	```
	Note the variable `TDSE_1D_SCRIPTS` is set in advance for the whole multiscale model. 
3. The dependency jobs are then submitted to the Slurm queue. 

## Single field caller
The single field caller is the option to run the C-TDSE on a single input from the CUPRAD field output without the Pythonic environment and MPI. 

**Note: the file `msg.tmp` containing the name of the preprocessed CUPRAD file with the TDSE input parameters must be located in the code execution directory!** If no `msg.tmp` file is available, the code exits with an error! The "msg.tmp" contains the HDF5 archive name on the first line:
```
CUPRAD_results_file.h5


```

First, the caller prints out the dimensions of the CUPRAD field array, e.g.:
```
Radial (r) dimension: 1024 
Propagation (z) dimension: 151
```
The caller then asks the user for the indices in the CUPRAD field array. For indices (512, 75) we type:
```
Set the index in the radial dimension: 512
Set the index in the propagation dimension: 75
```
The binary then computes the CTDSE given the input parameters in the preprocessed `CUPRAD_results_file.h5` and indices of the field. The output is stored in the new HDF5 file `results_(512,75).h5`.

## Python-TDSE wrapper
The Python-TDSE wrapper is an API that binds compiled C dynamic library, containing CTDSE functions, with the convenient Python interface. The methods and classes are located in the `PythonTDSE.py` file. 

Because the C language utilizes allocatable arrays and pointers, the approach inevitably comes with many caveats such as memory freeing, etc. The methods for loading the C arrays and freeing memory are implemented, however the user has to manually delete memory once access to the data is no longer required. 

A detailed explanation of the Python-TDSE wrapper is provided in the Jupyter Notebook file `PythonTDSE.ipynb`.


<!-- # Development

## Goals
TDSE is used as a part of the multi-scale model. It is also usefull itself for various solely microscopic studies. Furthemore, it might have more usages for the macroscopic studies: 1) to process numerical fields from CUPRADS, 2) to create a list of microscopic currents that are used to intepolate the HHG sources in a macroscopic medium.

The code shall be well-organised for all these tasks: core-rutines are written only once and are called from different programs. The architecture is the following: the solver itselfs works as a function `inputs` $\to$ `outputs`. This is called from the driving programs for each task.

We provide two opssible operations in the publication:
* MPI processing of the numerical field
* A single caller to treat microscopic problems

### Single TDSE for versatile applications
* Python wrapper to run the code easily from jupyter notebooks etc.
* Reintroduce storing the wavefunction as an output of TDSE for detailed analysis
* (?) Use Gabor transform from c, ...

## Improvements
* The single caller is separated in `single_caller.c` and `prop.c`. The former just unpacks and packs I/O structures to feed the caller. I think this is a bit clumsy and might be improved.
* The main part of the multi-scale model is the MPI scheduler. We need to comment and polish it.
	* The scheduler now works in a strided regime: zeroth processors gets tasks $0,N_{\text{procs}},2N_{\text{procs}},\dots$, first processor $1,N_{\text{procs}}+1,2N_{\text{procs}}+1,\dots$, ... (See `strided_scheduler*.c`.)
	* Current implementation uses a hdf5-output for each processor. Results are merged by Python code `post_processing/merge*.py`. We might use a collective access similarly to CUPRAD to store outputs directly.
	* The opearation still doesn't seem optimal. Some processors end their work much later than the others (the tasks are similar & by repated runs for the same configurations, it were different processors). It seem some subtasks are locked for some time (but not deadlocked). This was difficult to trace, the scale of the task was also a possible issue.
	* It seem to me that an elegant design would be to use a shared counter to assign tasks to idle processors. This functionality was introduced in the MPI3 standard as is provided as an example `NXTVAL` in the reference book[^1]. I was trying to implement it (see `hdf5+mpi_tuto/*`, `counting.c`, `counting2.c`), but it was unstable and problematic. We try to find someone experienced in MPI3 and discuss it.


[^1]: Gropp, Hoefler, THakur, Lusk *Using Advanced MPI*


# Minimalistic user-guide
This folder cointais a collection of *.c codes and a Python script used for

- MPI-scheduler that computes 1DTDSEs for all electric fields in an (r,z)-grid.
- Single 1DTDSE computed for one input.
- Python script collects all results from temporary files generated by the scheduler.

! It is still in experimental version. Mainly, the input units are not resolved. Next, it is hard-coded that code prints all available outputs, but the method for selecting some of them is already presented.

## MPI-scheduler
An example of a compilation and submission scripts:
<pre>
#!/bin/bash

#module purge
#module load intel intelmpi fftw3 hdf5

# ECLIPSE-ELI
#module purge
#module load GCC/6.1.0 OpenMPI/1.10.2-GCC-6.1.0-psm-dbg HDF5/1.8.13-OpenMPI-1.10.2-GCC-6.1.0-psm FFTW/3.3.5-OpenMPI-1.10.2-GCC-6.1.0-psm-dbg

# OCCIGEN
# module purge
module purge
module load intel/18.1 intelmpi/2018.1.163 hdf5/1.10.4
module load fftw/3.3.8
# module load intel/18.1 intelmpi/2018.1.163
# module load intel/19.4 intelmpi/2019.4.243
# module load intel/19.4 openmpi/intel/2.0.4
# module load gcc/8.3.0 openmpi/gnu/2.0.4

module list

rm *.h5 *.e sendjob *.output *.log

cp ../tx/results.h5 .
cp ../tx/results.h5 results2.h5

mv TDSEScheduler-*out oldlogs/
mv logfile.log-* oldlogs/

which h5pcc 
export HDF5_USE_SHLIB=yes # default static lib fore h5pcc
h5pcc -c $HOME/git/1DTDSE/constants.c $HOME/git/1DTDSE/tools_algorithmic.c $HOME/git/1DTDSE/tools_MPI-RMA.c $HOME/git/1DTDSE/tools_hdf5.c $HOME/git/1DTDSE/tridiag.c $HOME/git/1DTDSE/tools_fftw3.c $HOME/git/1DTDSE/tools.c $HOME/git/1DTDSE/prop.c $HOME/git/1DTDSE/singleTDSE.c $HOME/git/1DTDSE/strided_scheduler_separated_outs.c -L$HDF5_LIBDIR -lhdf5 -lhdf5_hl -lfftw3
h5pcc *.o -L$HDF5_LIBDIR -lhdf5 -lhdf5_hl -lm -lfftw3 -o test_strided2.e
rm *.o


echo 'compilation finished'
</pre>

<pre>
#!/bin/bash
#SBATCH -J 1DTDSE_MPI
#SBATCH  --constraint=HSW24  
#SBATCH  --exclusive
#SBATCH  --nodes=1
#SBATCH  --ntasks=2
#SBATCH  --ntasks-per-node=24 
#SBATCH  --threads-per-core=1
#SBATCH  --time=00:30:00 
#SBATCH  --output=TDSEScheduler-%J.out
module purge
module load intel/18.1 intelmpi/2018.1.163 hdf5/1.10.4 
module load fftw/3.3.8
# module load intel/19.4 intelmpi/2019.4.243
# module load intel/19.4 openmpi/intel/2.0.4
# module load gcc/8.3.0 openmpi/gnu/2.0.4

export I_MPI_PIN_PROCESSOR_LIST=allcores:map=spread
export SLURM_CPU_BIND=none
#
export I_MPI_DAPL_PROVIDER=ofa-v2-mlx5_0-1s
export I_MPI_DAPL_DIRECT_COPY_THRESHOLD=655360
# 
export I_MPI_PIN=1
export I_MPI_DEBUG=6

ldd ./test_strided2.e

time  mpirun -n $SLURM_NTASKS ./test_strided2.e > logfile.log-$SLURM_JOBID
</pre>

## Single 1DTDSE
The main feature is that this code shares the source with MPI. It can thus be used for development tuning that is then directly available in MPI, e.g. I used valgrind for this code to check that there are no internal leaks within the procedure (all possible problems are then intrinsic to the MPI-Scheduler).
<pre>
#!/bin/bash

#module purge
#module load intel intelmpi fftw3 hdf5

# ECLIPSE-ELI
#module purge
#module load GCC/6.1.0 OpenMPI/1.10.2-GCC-6.1.0-psm-dbg HDF5/1.8.13-OpenMPI-1.10.2-GCC-6.1.0-psm FFTW/3.3.5-OpenMPI-1.10.2-GCC-6.1.0-psm-dbg

# OCCIGEN
module purge
module load intel/18.1 intelmpi/2018.1.163 hdf5/1.10.4
module load fftw/3.3.8

rm *.h5 *.e sendjob *.output *.log results/*

cp ../tx/results_single_short.h5 results.h5
cp ../tx/results_single_short.h5 results2.h5
which h5pcc 
export HDF5_USE_SHLIB=yes # default static lib fore h5pcc
h5pcc  -c $HOME/git/1DTDSE/constants.c $HOME/git/1DTDSE/tools_hdf5.c $HOME/git/1DTDSE/tridiag.c $HOME/git/1DTDSE/tools.c $HOME/git/1DTDSE/tools_fftw3.c $HOME/git/1DTDSE/tools_algorithmic.c $HOME/git/1DTDSE/prop.c $HOME/git/1DTDSE/singleTDSE.c $HOME/git/1DTDSE/single_caller.c -L$HDF5_LIBDIR -lhdf5 -lhdf5_hl -lfftw3 -g
h5pcc --echo  *.o -L$HDF5_LIBDIR -lhdf5 -lhdf5_hl -lm -lfftw3 -g -o test_s1.e
rm *.o


echo 'compilation finished'
</pre>

It can be run in an interactive mode using
<pre>
module load intel/18.1 intelmpi/2018.1.163 hdf5/1.10.4; module load fftw/3.3.8

srun test_s1.e
</pre>


## Python post-processor
Because there is no stable direct MPI-Scheduler, there is a post-processor connectiong all files, it is in the folder **post_processing**. It is general and it can be easily modiable for other purposes.

At the instant, it uses hdf5-oprations that are sourced from *mynumerics.py* from the Hankel-transform repo.



# Development notes
## The plot of the code:

1. See the comment at the end of the file for deatils about the MPI-scheduler.

2. The main program (MP) reads all the parameters from an input HDF5-archive (each process independently, read-only).

3. MP decides based on parameters the type of input field. Fist implementation: stick to numerical fields from CUPRAD stored in the archive.

4. MPI-scheduler executes a simulation in every point. THe data are directly written into the output using the mutex.

5. The code finishes.

- DEVELOPMENT: wrap the call of singleTDSE, where propagation is called to test, erase this extra step after

!!! There is memory allocated in every TDSE. Check there are no memory leaks, it matters now.

## Local problems:
single_caller.c is a program to call a single TDSE simulation. The input is one numerical field at the instant.


## Extensions/features already presented in 1DTDSE
THere is a list of features we added in the code throughout the time, we don't have them in the sheduler. But it would be worthy to re-introduce them simply by modifying the output structure.

- The features already presented in TDSE:
	1. Print wavefunction (WFT)
	2. Print Gabor transformation
	3. Photoelectron spectrum (PeS, Fabrice)
  4. Various outputs modifications (e.g. ionisation filtering done for Sophie)

Gabor is computationally demanding; WF and PeS are both data-storage demandig. It should be used then only in few prescribed points.


--There is possibility of various inputs:
	1. Numerical/analytic field
	2. Computation on velocity/length gauge

We have already an analytic model of a beam (Python/MATLAB), we will then rewrite it and construct the parameters on-the-fly.
The versatility in numeric-or-analytic field length-or-velocity gauge is ideal for testing of numerical vector potential that we can use after in SFA.


## Development notes:
<pre>
0) Use void*-types for inputs and link it with the required ouput precision.

1) We can use checks whether parameters exist in the input HDF5-archive. If not, we can create them with default values.
Implementation: since this is I/O operation with one file, we need r/w. Maybe read paramc only by proc 0 and the broadcast structure (see the MPI book for transfering structs).

For reading, it should be easy. ** R/W may occur simultaneously in in the MPI loop. Separate I/O at the instant or ensure it will work (R/W from independent datasets may be fine???).
https://support.hdfgroup.org/HDF5/Tutor/selectsimple.html

2) Actual construction does some preparations within 1 TDSE simulation, it is desirable to separate preparations and core propagation, it's then more versatile for the MPI-calling.
2.solution) separate all preparatory work from singleTDSE into one procedure, called pecedently the calculations, the "pure" propagation then wites into its template.
2.develop) keep in mind that this approach may not be possible every time and one run has to be done before alligning it... Try to code it clever way.

3) The code is inconsistent. SOme outputs from the first version are listed as independent variables and not encapsulated in structures. Fix it.

4) There is a "better" mutex proposed in the MPI3-book, ref. [44] therein. Try to implement it.
4.add) The shared mutex seems to be wrong by some reason.

5) we get rid of mutexes and use rather parallel acces to files all the time.
5.develop) it seems that many-readers many-writers would be possible by HDF5 parallel since we will not modify the file much. However, we may also try stick with independent files and eventually 
https://stackoverflow.com/questions/49851046/merge-all-h5-files-using-h5py
https://portal.hdfgroup.org/display/HDF5/Collective+Calling+Requirements+in+Parallel+HDF5+Applications
</pre>

## Valgrind
<pre>
	  for(i=0;i<=num_r;i++)
	  {
		  dinfnew[2*i] = dinf[2*i] - Eguess/12. + potential(x[i],trg)/12.; dinfnew[2*i+1] = dinf[2*i+1];
		  dnew[2*i] = 10*potential(x[i],trg)/12.+ d[2*i] - 10*Eguess/12.; dnew[2*i+1] = d[2*i+1];
		  dsupnew[2*i] = dsup[2*i] - Eguess/12. + potential(x[i+1],trg)/12.; dsupnew[2*i+1] = dsup[2*i+1];
		  diag[2*i] = potential(x[i],trg)+ d[2*i] ; diag[2*i+1] = d[2*i+1];
	  }
</pre>
wrongly specified x(n+1) element. Hotfixed.

<pre>
		for(j = 0 ; j<= num_r ; j++) 
		{	
			dinfnew1[2*j] = 1/12.; dinfnew1[2*j+1] = 0.5*dt*( -0.5/(dx*dx) )+0.5*dt*1/12.*(cpot*potential(x[j],trg));
			dnew1[2*j] = 10/12.; dnew1[2*j+1] = 0.5*dt*( 1./(dx*dx) )+0.5*dt*10/12.*(cpot*potential(x[j],trg));
			//dsupnew1[2*j] = 1/12.; dsupnew1[2*j+1] = 0.5*dt*( -0.5/(dx*dx) )+0.5*dt*1/12.*(cpot*potential(x[j+1],trg));			
		}
</pre>
dtto


<pre>
Efield.Field = FourInterp(k1, Efield.Field, Efield.Nt); // make the interpolation !!!!!! tgrid does not correspond any more
</pre>
tricky since a pointer may be lost

-->
